{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 1. Import the data\n",
    "  We start with importing the two given datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"fake_or_real_news_training.csv\")\n",
    "test = pd.read_csv(\"fake_or_real_news_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10498</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2439</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0  10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "1   2439  Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "\n",
       "                                                text  \n",
       "0  September New Homes Sales Rise Back To 1992 Le...  \n",
       "1  But when Congress debated and passed the Patie...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we visualize some rows of the dataset to get a feel of its structure.\n",
    "test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0   8476                       You Can Smell Hillary’s Fear   \n",
       "1  10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "\n",
       "                                                text label   X1   X2  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  NaN  NaN  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  NaN  NaN  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Here we visualize some rows of the dataset to get a feel of its structure.\n",
    "train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here, we will fill NAs in the last two columns of the training dataset, which we know to be often empty.\n",
    "train['X1'] = train['X1'].fillna(\"\")\n",
    "train['X2'] = train['X2'].fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data preparation\n",
    "In this section, we will prepare the data so that it is in a suitable format to be analyzed and processed. We start by splitting the training set into training and test. After this first step, we use BeautifulSoup to clean the text from unwanted characters. After this we use the CountVectorizer to create a bag of words (BOW) on the one side, and on the other side, using the TF-IDF Vectorizer we create the weights for every token in the dataset, so that we can have two different inputs for our modelling stage.\n",
    "\n",
    "As for the train and test split, we chose not to create a validation set as we wanted to keep the highest possible amount of observations for training the models (the test set is not labelled so we can't use that one either)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we split the dataset into training and test, and from the test set we omit columns we dont need. \n",
    "X_train = train['text'] + train['title'] + train['X1'] + train['X2'] \n",
    "X_test = test['text'] + test['title']\n",
    "y_train = train.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data cleaning\n",
    "BeautifulSoup is what will help us here to clean the text from unwanted characters, namely punctuation, html tags and numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defing the function to keep only the letters.\n",
    "def cleaning(text):\n",
    "    soup = BeautifulSoup(\n",
    "        text\n",
    "    )  # defining a BeautifulSoup object, which represents the document as a nested data structure:\n",
    "    text = soup.get_text()  # extracting all the text\n",
    "    return re.sub(\"[^A-Za-z ]+\", \"\", str(text))  # return the cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.apply(cleaning)\n",
    "X_test = X_test.apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)\n",
    "X_train.columns = ['text']\n",
    "X_test.columns = ['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Daniel Greenfield a Shillman Journalism Fellow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US Secretary of State John F Kerry said Monday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kaydee King KaydeeKing November   The lesson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Its primary day in New York and frontrunners H...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Daniel Greenfield a Shillman Journalism Fellow...\n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...\n",
       "2  US Secretary of State John F Kerry said Monday...\n",
       "3   Kaydee King KaydeeKing November   The lesson ...\n",
       "4  Its primary day in New York and frontrunners H..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we make the whole text lowercase to allow the models to reach better scores.\n",
    "X_train['text'] = [entry.lower() for entry in X_train['text']]\n",
    "X_test['text'] = [entry.lower() for entry in X_test['text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>daniel greenfield a shillman journalism fellow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google pinterest digg linkedin reddit stumbleu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us secretary of state john f kerry said monday...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>kaydee king kaydeeking november   the lesson ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>its primary day in new york and frontrunners h...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  daniel greenfield a shillman journalism fellow...\n",
       "1  google pinterest digg linkedin reddit stumbleu...\n",
       "2  us secretary of state john f kerry said monday...\n",
       "3   kaydee king kaydeeking november   the lesson ...\n",
       "4  its primary day in new york and frontrunners h..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = X_train.append(X_test)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Bag of Words\n",
    "In this part, we preprocess the data so that it is in a suitable format to be processed by the algorithms we will use later on.\n",
    "Bag of Words (BOW) is a method to extract features from text documents. These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set (Source: freecodecamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/andrea_salvati/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['me', 'my', 'myself', 'we']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words(\"english\")[1:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6320, 48635)\n"
     ]
    }
   ],
   "source": [
    "#Here we apply the Vectorizer to the dataset.\n",
    "vectorizer = CountVectorizer(min_df=2, stop_words=\"english\")\n",
    "vectorizer.fit(dataset['text'])\n",
    "\n",
    "dataset_bw = vectorizer.transform(dataset['text'])\n",
    "\n",
    "print(dataset_bw.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Tf-IDF weighting\n",
    "Here we apply a different vectorizer, so that we have two different inputs and we can then compare the results of the two techniques. We try however a third (not completely new as it is still basing on TF_IDF vectorizer) version of the input: we try and adopt the n-gram technique. An n-gram is a contiguous sequence of n items from a given sample of text or speech (Wikipedia). The purpose of adding this particular technique is that it is supposed to make results better by retrieving lexical and semantic information from the data.\n",
    "\n",
    "Term Frequency (TF) gives us the frequency of the word in each document in the corpus. It is the ratio of number of times the word appears in a document compared to the total number of words in that document (Source: freecodecamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6320, 48635)\n"
     ]
    }
   ],
   "source": [
    "#Here we apply the vectorizer to the dataset.\n",
    "tfidf = TfidfVectorizer(min_df=2, stop_words=\"english\")\n",
    "tfidf.fit(dataset['text'])\n",
    "\n",
    "dataset_tfidf = tfidf.transform(dataset['text'])\n",
    "\n",
    "print(dataset_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6320, 444384)\n"
     ]
    }
   ],
   "source": [
    "# Here we apply the vectorizer together with n-grams\n",
    "ng = TfidfVectorizer(min_df=2, stop_words=\"english\", ngram_range=(1,3))\n",
    "ng.fit(dataset['text'])\n",
    "\n",
    "dataset_tfidf_ng = ng.transform(dataset['text'])\n",
    "\n",
    "print(dataset_tfidf_ng.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Definition of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_len: 3999\n",
      "test_len: 2321\n"
     ]
    }
   ],
   "source": [
    "print(\"train_len: \" + str(len(X_train)))\n",
    "print(\"test_len: \" + str(len(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we re-split the data into training and test after we have prepared it and cleaned it for the TF-IDF approach\n",
    "X_train_tfidf = dataset_tfidf[:3999]\n",
    "X_test_tfidf = dataset_tfidf[3999:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we re-split the data into training and test after we have prepared it and cleaned it for the BOW approach\n",
    "X_train_bw = dataset_bw[:3999]\n",
    "X_test_bw = dataset_bw[3999:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we re-split the data into training and test after we have prepared it and cleaned it for the TF_IDF + n-gram approach\n",
    "X_train_tfidf_ng = dataset_tfidf_ng[:3999]\n",
    "X_test_tfidf_ng = dataset_tfidf_ng[3999:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Lemmatization\n",
    "Lemmatization helps to achieve greater precision in the modelling stage as it considers the root of the word and not the word itself.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lem = X_train\n",
    "X_test_lem = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://medium.com/@bedigunjit/simple-guide-to-text-classification-nlp-using-svm-and-naive-bayes-with-python-421db3a72d34\n",
    "def lemmatization(dataset):\n",
    "    # Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "    dataset['text'] = [word_tokenize(entry) for entry in dataset['text']]\n",
    "    # Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "    # WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\n",
    "    tag_map = defaultdict(lambda: wn.NOUN)\n",
    "    tag_map[\"J\"] = wn.ADJ\n",
    "    tag_map[\"V\"] = wn.VERB\n",
    "    tag_map[\"R\"] = wn.ADV\n",
    "    for index, entry in enumerate(dataset['text']):\n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "                word_Final = word_Lemmatized.lemmatize(word, tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "        # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "        dataset.loc[index, 'text'] = str(Final_words)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lem = lemmatization(X_train_lem)\n",
    "X_test_lem = lemmatization(X_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['daniel', 'greenfield', 'a', 'shillman', 'jou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['google', 'pinterest', 'digg', 'linkedin', 'r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['u', 'secretary', 'of', 'state', 'john', 'f',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['kaydee', 'king', 'kaydeeking', 'november', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['it', 'primary', 'day', 'in', 'new', 'york', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  ['daniel', 'greenfield', 'a', 'shillman', 'jou...\n",
       "1  ['google', 'pinterest', 'digg', 'linkedin', 'r...\n",
       "2  ['u', 'secretary', 'of', 'state', 'john', 'f',...\n",
       "3  ['kaydee', 'king', 'kaydeeking', 'november', '...\n",
       "4  ['it', 'primary', 'day', 'in', 'new', 'york', ..."
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_lem = X_train_lem.append(X_test_lem)\n",
    "dataset_lem.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6320, 40757)\n"
     ]
    }
   ],
   "source": [
    "#Here we apply the vectorizer to the dataset.\n",
    "tfidf = TfidfVectorizer(min_df=2)\n",
    "tfidf.fit(dataset_lem['text'])\n",
    "dataset_tfidf_lem = tfidf.transform(dataset_lem['text'])\n",
    "\n",
    "print(dataset_tfidf_lem.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we re-split the data into training and test after we have prepared it and cleaned it for the TF_IDF + n-gram approach\n",
    "X_train_tfidf_lem = dataset_tfidf_lem[:3999]\n",
    "X_test_tfidf_lem = dataset_tfidf_lem[3999:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Feature Selection\n",
    "In this section, we take care of selecting only the most important features in the hope to make computation more efficient and to make our results better. To do so, we adopt a Chi-Squared test, to avoid taking into consideration features that are correlated, as the chi-square test is a statistical test of independence to determine the dependency of two variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we try to optimize the K parameter for the NB algorithm\n",
    "def chi_optimization(dataset, k, estimator):\n",
    "    from sklearn import naive_bayes\n",
    "    k = k\n",
    "    results = []\n",
    "\n",
    "    for i in k:\n",
    "        selector = SelectKBest(chi2, k=i)\n",
    "        selector.fit(dataset, y_train)\n",
    "        top_words = selector.get_support().nonzero()\n",
    "\n",
    "        # Pick only the most informative columns in the data.\n",
    "        X_train_chi = dataset[:,top_words[0]]\n",
    "\n",
    "        #Here we insert the TF-IDF processed data and we train our model\n",
    "        from sklearn import naive_bayes\n",
    "\n",
    "        naive_bayes = naive_bayes.MultinomialNB()\n",
    "        naive_bayes.fit(X_train_chi, y_train)\n",
    "\n",
    "        #Here we get the metrics for the model\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        accuracy_chi = cross_val_score(estimator=estimator, X=X_train_chi, y=y_train, cv=5)\n",
    "        results.append(max(accuracy_chi).round(3))\n",
    "        \n",
    "    opt = {\"k\":k, \"results\":results}\n",
    "    opt = pd.DataFrame(opt)\n",
    "    \n",
    "    best_k = int(opt.loc[opt[\"results\"].idxmax()][\"k\"])\n",
    "    print ('the best k is: ' + str(best_k))\n",
    "    return best_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best k is: 3050\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3050"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_optimization(X_train_tfidf, np.arange(3000,3500,50),naive_bayes.MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 1000 most informative columns for tfidf\n",
    "selector = SelectKBest(chi2, k=3050)\n",
    "selector.fit(X_train_tfidf, y_train)\n",
    "top_words = selector.get_support().nonzero()\n",
    "\n",
    "# Pick only the most informative columns in the data.\n",
    "X_train_tfidf_chi = X_train_tfidf[:,top_words[0]]\n",
    "X_test_tfidf_chi = X_test_tfidf[:,top_words[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best k is: 13000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13000"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_optimization(X_train_tfidf_lem, np.arange(12000,20000,1000),LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 1000 most informative columns for tfidf\n",
    "selector = SelectKBest(chi2, k=13000)\n",
    "selector.fit(X_train_tfidf_lem, y_train)\n",
    "top_words = selector.get_support().nonzero()\n",
    "\n",
    "# Pick only the most informative columns in the data.\n",
    "X_train_lem_chi = X_train_tfidf_lem[:,top_words[0]]\n",
    "X_test_lem_chi = X_test_tfidf_lem[:,top_words[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best k is: 3500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3500"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_optimization(X_train_tfidf_lem, np.arange(2000,5000,500),LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the 1000 most informative columns for tfidf\n",
    "selector = SelectKBest(chi2, k=3500)\n",
    "selector.fit(X_train_tfidf_lem, y_train)\n",
    "top_words = selector.get_support().nonzero()\n",
    "\n",
    "# Pick only the most informative columns in the data.\n",
    "X_train_lem_chi_me = X_train_tfidf_lem[:,top_words[0]]\n",
    "X_test_lem_chi_me = X_test_tfidf_lem[:,top_words[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modelling\n",
    "In this section, we start the modeling task. We will experiment with different models, namely: Nayve Bayes, SVM, MaxEnt aka LogReg and in the end we try to put the steps together in a pipeline. The procedure we will follow is to input the three versions of the input dataset (BOW, TF-IDF, TF-IDF with n-grams) and train three models based on the sets. After this, we will test the models, to see which input provides best training and hence best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Naive Bayes\n",
    "In machine learning, naive Bayes classifiers are a family of simple \"probabilistic classifiers\" based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. (Source: Wikipedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import naive_bayes, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.854\n"
     ]
    }
   ],
   "source": [
    "#Here we start with inputting the TF-IDF processed data\n",
    "naive_bayes = naive_bayes.MultinomialNB()\n",
    "naive_bayes.fit(X_train_tfidf, y_train)\n",
    "\n",
    "predictions_nb_tfidf = naive_bayes.predict(X_test_tfidf)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_nb_tfidf = cross_val_score(\n",
    "    estimator=naive_bayes, X=X_train_tfidf, y=y_train, cv=5\n",
    ")\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_nb_tfidf).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.897\n"
     ]
    }
   ],
   "source": [
    "#Here we start with inputting the BOW processed data\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "naive_bayes = naive_bayes.MultinomialNB()\n",
    "naive_bayes.fit(X_train_bw, y_train)\n",
    "\n",
    "predictions_nb_bw = naive_bayes.predict(X_test_bw)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_nb_bw = cross_val_score(estimator=naive_bayes, X=X_train_bw, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_nb_bw).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.918\n"
     ]
    }
   ],
   "source": [
    "#Here we start with inputting the TF-IDF + Chi2 processed data\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "naive_bayes = naive_bayes.MultinomialNB()\n",
    "naive_bayes.fit(X_train_tfidf_chi, y_train)\n",
    "\n",
    "predictions_nb_tfidf_chi = naive_bayes.predict(X_test_tfidf_chi)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_nb_tfidf_chi = cross_val_score(estimator=naive_bayes, X=X_train_tfidf_chi, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_nb_tfidf_chi).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.826\n"
     ]
    }
   ],
   "source": [
    "#Here we start with inputting the TF-IDF + Chi2 processed data\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "naive_bayes = naive_bayes.MultinomialNB()\n",
    "naive_bayes.fit(X_train_tfidf_lem, y_train)\n",
    "\n",
    "predictions_nb_tfidf_lem = naive_bayes.predict(X_test_tfidf_lem)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_nb_tfidf_lem = cross_val_score(estimator=naive_bayes, X=X_train_tfidf_lem, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_nb_tfidf_lem).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.808\n"
     ]
    }
   ],
   "source": [
    "#Here we start with inputting the TF-IDF + n-gram processed data\n",
    "from sklearn import naive_bayes\n",
    "\n",
    "naive_bayes = naive_bayes.MultinomialNB()\n",
    "naive_bayes.fit(X_train_tfidf_ng, y_train)\n",
    "\n",
    "\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_nb_tfidf_ng = cross_val_score(estimator=naive_bayes, X=X_train_tfidf_ng, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_nb_tfidf_ng).round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 SVM\n",
    "Support vector machines are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training examples, each marked as belonging to one or the other of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, making it a non-probabilistic binary linear classifier (Source: Wikipedia)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.933\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the classifier\n",
    "SVM = LinearSVC()\n",
    "SVM.fit(X_train_tfidf,y_train)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM_tfidf = SVM.predict(X_test_tfidf)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_svm_tfidf = cross_val_score(estimator=SVM, X=X_train_tfidf, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_svm_tfidf).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.875\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the classifier\n",
    "SVM = LinearSVC()\n",
    "SVM.fit(X_train_bw,y_train)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM_bw = SVM.predict(X_test_bw)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_svm_bw = cross_val_score(estimator=SVM, X=X_train_bw, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_svm_bw).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.927\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the classifier\n",
    "SVM = LinearSVC()\n",
    "SVM.fit(X_train_tfidf_ng, y_train)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM_ng = SVM.predict(X_test_tfidf_ng)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_svm_tfidf_ng = cross_val_score(estimator=SVM, X=X_train_tfidf_ng, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_svm_tfidf_ng).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.933\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "# fit the training dataset on the classifier\n",
    "SVM = LinearSVC()\n",
    "SVM.fit(X_train_tfidf_lem, y_train)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_SVM_lem = SVM.predict(X_test_tfidf_lem)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_svm_tfidf_lem = cross_val_score(estimator=SVM, X=X_train_tfidf_lem, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_svm_tfidf_lem).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.947\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "# fit the training dataset on the classifier\n",
    "SVM = LinearSVC()\n",
    "SVM.fit(X_train_lem_chi, y_train)\n",
    "\n",
    "# predict the labels on validation dataset\n",
    "predictions_svm_lem_chi = SVM.predict(X_test_lem_chi)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_svm_lem_chi = cross_val_score(estimator=SVM, X=X_train_lem_chi, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_svm_lem_chi).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 SVM Optimized\n",
    "Grid search builds a model for every combination of hyperparameters specified and evaluates each model. This is the approach we pick to optimze the hyperparameter of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': array([0.1 , 0.15, 0.2 , 0.25, 0.3 , 0.35, 0.4 , 0.45, 0.5 , 0.55, 0.6 ,\n",
      "       0.65, 0.7 , 0.75, 0.8 , 0.85, 0.9 , 0.95, 1.  , 1.05, 1.1 , 1.15,\n",
      "       1.2 , 1.25])}\n"
     ]
    }
   ],
   "source": [
    "# Create regularization hyperparameter space. After several trial, it was possible to see that the best C was around 0.1.\n",
    "# In order to reduce the computation time I build the C space around that value\n",
    "C = np.arange(0.1, 1.3, 0.05)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C)\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 1.2500000000000004\n"
     ]
    }
   ],
   "source": [
    "clf_SVM = GridSearchCV(SVM, hyperparameters, cv=5, verbose=0)\n",
    "best_model = clf_SVM.fit(X_train_lem_chi, y_train)\n",
    "print(\"Best C:\", best_model.best_estimator_.get_params()[\"C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.9458\n"
     ]
    }
   ],
   "source": [
    "# optimizing my SVM Model\n",
    "SVM_GS = LinearSVC(C=best_model.best_estimator_.get_params()[\"C\"])\n",
    "SVM_GS.fit(X_train_lem_chi,y_train)\n",
    "\n",
    "y_pred_GS_SVM_tfidf_lem = SVM_GS.predict(X_test_lem_chi)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_GS_SVM_lem_chi = cross_val_score(estimator=SVM_GS, X=X_train_lem_chi, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_GS_SVM_lem_chi).round(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 MaxEnt\n",
    "Here we want to adopt the MaxEnt model, aka the Logistic Regression (or logit model) which is a widely used statistical model that in its basic form uses a logistic function to model a binary dependent variable. (Source: Techopedia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.907\n"
     ]
    }
   ],
   "source": [
    "# defing my logistic Regression Model, input as BOW\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_bw, y_train)\n",
    "y_pred_logreg_bw = logreg.predict(X_test_bw)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_logreg_bw = cross_val_score(estimator=logreg, X=X_train_bw, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_logreg_bw).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.907\n"
     ]
    }
   ],
   "source": [
    "# defing my logistic Regression Model, input processed with TF-IDF\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_tfidf,y_train)\n",
    "y_pred_logreg_tfidf = logreg.predict(X_test_tfidf)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_logreg_tfidf = cross_val_score(estimator=logreg, X=X_train_tfidf, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_logreg_tfidf).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.913\n"
     ]
    }
   ],
   "source": [
    "# defing my logistic Regression Model, input processed with TF-IDF\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_tfidf_lem,y_train)\n",
    "y_pred_logreg_tfidf = logreg.predict(X_test_tfidf_lem)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_logreg_tfidf_lem = cross_val_score(estimator=logreg, X=X_train_tfidf_lem, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_logreg_tfidf_lem).round(3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.914\n"
     ]
    }
   ],
   "source": [
    "# defing my logistic Regression Model, input processed with TF-IDF\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_lem_chi_me,y_train)\n",
    "\n",
    "y_pred_logreg_tfidf = logreg.predict(X_test_lem_chi_me)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_logreg_lem_chi = cross_val_score(estimator=logreg, X=X_train_lem_chi_me, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_logreg_lem_chi).round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 MaxEnt Optimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model, datasets\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': array([11.  , 11.25, 11.5 , 11.75, 12.  , 12.25, 12.5 , 12.75, 13.  ])}\n"
     ]
    }
   ],
   "source": [
    "# Create regularization hyperparameter space. After several trial, it was possible to see that the best C was around 0.1.\n",
    "# In order to reduce the computation time I build the C space around that value\n",
    "C = np.arange(11, 13.25, 0.25)\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C)\n",
    "\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C: 12.0\n"
     ]
    }
   ],
   "source": [
    "#here we train the model with the TF-IDF processed input\n",
    "clf_ME = GridSearchCV(logreg, hyperparameters, cv=5, verbose=0)\n",
    "best_model = clf_ME.fit(X_train_lem_chi_me, y_train)\n",
    "print(\"Best C:\", best_model.best_estimator_.get_params()[\"C\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.941\n"
     ]
    }
   ],
   "source": [
    "# optimizing my logistic Regression Model with the optimal hyperparameters\n",
    "logreg_GS = LogisticRegression(C=best_model.best_estimator_.get_params()[\"C\"])\n",
    "logreg_GS.fit(X_train_lem_chi_me, y_train)\n",
    "y_pred_GS_logreg_tfidf = logreg_GS.predict(X_test_lem_chi_me)\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy_GS_logreg_lem_chi = cross_val_score(estimator=logreg_GS, X=X_train_lem_chi_me, y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy_GS_logreg_lem_chi).round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Pipeline\n",
    "Here we try to stack different steps of the process in a pipeline, so to execute more efficiently and in a more automated fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy with cross validation: 0.932\n"
     ]
    }
   ],
   "source": [
    "# this calculates a vector of term frequencies for\n",
    "# each document\n",
    "vect = CountVectorizer()\n",
    "\n",
    "# this normalizes each term frequency by the\n",
    "# number of documents having that term\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "# this is a linear SVM classifier\n",
    "clf = LinearSVC()\n",
    "\n",
    "pipeline = Pipeline([(\"vect\", vect), (\"tfidf\", tfidf), (\"clf\", clf)])\n",
    "\n",
    "# call fit as you would on any classifier\n",
    "pipeline.fit(X_train['text'], y_train)\n",
    "\n",
    "# predict test instances\n",
    "pred_pip = pipeline.predict(X_test['text'])\n",
    "\n",
    "# calculate f1\n",
    "warnings.simplefilter(\"ignore\")\n",
    "accuracy = cross_val_score(estimator=pipeline, X=X_train['text'], y=y_train, cv=5)\n",
    "print(\"accuracy with cross validation: \" + str(max(accuracy).round(3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Create CSV with submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = {\n",
    "    \"best_Naive_Bayes\": max(accuracy_nb_tfidf_chi).round(2),\n",
    "    \"best_SVM\": max(accuracy_svm_lem_chi).round(2),\n",
    "    \"best_MaxEnt\": max(accuracy_GS_logreg_lem_chi).round(2),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>pre_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10498</td>\n",
       "      <td>September New Homes Sales Rise——-Back To 1992 ...</td>\n",
       "      <td>September New Homes Sales Rise Back To 1992 Le...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2439</td>\n",
       "      <td>Why The Obamacare Doomsday Cult Can't Admit It...</td>\n",
       "      <td>But when Congress debated and passed the Patie...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>864</td>\n",
       "      <td>Sanders, Cruz resist pressure after NY losses,...</td>\n",
       "      <td>The Bernie Sanders and Ted Cruz campaigns vowe...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4128</td>\n",
       "      <td>Surviving escaped prisoner likely fatigued and...</td>\n",
       "      <td>Police searching for the second of two escaped...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>662</td>\n",
       "      <td>Clinton and Sanders neck and neck in Californi...</td>\n",
       "      <td>No matter who wins California's 475 delegates ...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID                                              title  \\\n",
       "0  10498  September New Homes Sales Rise——-Back To 1992 ...   \n",
       "1   2439  Why The Obamacare Doomsday Cult Can't Admit It...   \n",
       "2    864  Sanders, Cruz resist pressure after NY losses,...   \n",
       "3   4128  Surviving escaped prisoner likely fatigued and...   \n",
       "4    662  Clinton and Sanders neck and neck in Californi...   \n",
       "\n",
       "                                                text pre_label  \n",
       "0  September New Homes Sales Rise Back To 1992 Le...      FAKE  \n",
       "1  But when Congress debated and passed the Patie...      REAL  \n",
       "2  The Bernie Sanders and Ted Cruz campaigns vowe...      REAL  \n",
       "3  Police searching for the second of two escaped...      REAL  \n",
       "4  No matter who wins California's 475 delegates ...      REAL  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['pre_label'] = predictions_svm_lem_chi\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = test[['ID','pre_label']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will hereby quickly summarize the flow of the work. The main steps we took will be briefly described below.\n",
    "At first, we imported the data and started preparing it to be suitable to analysis (after having performed some EDA).\n",
    "The main steps in the preparation were cleaning and preprocessing using vectorizers to get the data into suitable format for \n",
    "processing it with the chosen algorithms. The cleaning was performed for the most using bs4 BeatifulSoup.\n",
    "\n",
    "Before the modelling phase though, there are two more steps, lemmatization being the first one. Lemmatization helps to achieve greater precision in the modelling stage as it considers the root of the word and not the word itself. On top of this, we decided to adopt a Chi Square test for feature selection so to make processing leaner and more efficient, while isolating noise and achieving better results.  \n",
    "\n",
    "The modelling phase consisted of iteratively running the same models multiple times with different input data (in terms of preprocessing) to discover what treatment generated the best results. \n",
    "Naive Bayes, Support Vector Machines and Logistic Regression (MaxEnt) were the models we adopted to classify the news as fake or real. The best results were achieved while running the optimized SVM model where the accuracy was as high as 0.9458, when the input was the data preprocessed through lemmatization, Chi Square and TF-IDF. Below a summary of the results achieved in the different models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
